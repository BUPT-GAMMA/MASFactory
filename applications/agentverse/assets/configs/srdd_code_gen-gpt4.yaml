cnt_agents: &cnt_agents 4
max_turn: &max_turn 2

prompts:
  role_assigner_prepend_prompt: &role_assigner_prepend_prompt |-
    # Role Description
    You are the leader of a group of experts. Recruit a small group of experts with diverse identity to correctly implement the following software:
    {task_description}
    
    You can recruit {cnt_critic_agents} expert in different fields. What experts will you recruit to better generate an accurate solution?

    Here are some suggestion:
    {advice}
    
  role_assigner_append_prompt: &role_assigner_append_prompt |-
    # Response Format Guidance
    You should respond with a list of expert descriptions. For example:
    1. a backend engineer specialized in ...
    2. a software architect who is good at ...
    3. a QA engineer experienced with ...
    ...

    Only respond with the description of each role. Do not include your reason.

  solver_prepend_prompt: &solver_prepend_prompt |-
    Implement the following software requirement description in Python:
    {task_description}

    # Previous Solution
    The solution you gave in the last step is:
    {previous_plan}

    # Evaluation
    The unit testing/evaluation feedback below may be noisy or incorrect; use it as a hint, but prioritize the task specification:
    {advice}

    # Critics
    The following messages are critics' feedback (may also contain mistakes). Address them only when they are consistent with the task specification:
    {criticisms}
  
  solver_append_prompt: &solver_append_prompt |-
    You are {role_description}. Provide a correct and self-contained Python implementation in a single file.
    Requirements:
    - Use only the Python standard library.
    - No internet access.
    - Do NOT run interactive I/O at import time (put CLI under `if __name__ == "__main__":`).
    - The program must run non-interactively when executed as `python main.py` (stdin may be EOF). Do NOT use `input()`. If you must handle EOF, exit gracefully (e.g., `return` / `sys.exit(0)`) or use defaults.
    - Do NOT use placeholder stubs like `pass`, `TODO`, or `...` anywhere in the code. If you catch an exception, handle it (e.g., return/raise/log/use defaults) instead of swallowing it with a no-op. Also avoid comments containing the word "pass" (e.g., "passing") because the completeness checker treats it as unfinished.
    - Make the code testable (prefer pure functions/classes).

    Your response MUST contain only Python code. Use ```python to wrap the full file content.

  critic_prepend_prompt: &critic_prepend_prompt |-
    You are in a discussion group, aiming to implement the following software:
    {task_description}

  critic_append_prompt: &critic_append_prompt |-
    You are {role_description}. Based on your knowledge, can you check the functional correctness of the latest implementation given below?
    ```python
    {solution}
    ```
    When responding, follow these rules:
    1. If the latest provided solution is correct, end your response with a special token "[Agree]".
    2. If the solution is incorrect, give your comment and end your response with a special token "[Disagree]".
    3. Do NOT include any code blocks in your response.
    4. The LAST line of your response MUST be exactly "[Agree]" or "[Disagree]" (no extra text after it).
    5. If the code contains placeholder stubs like `pass`, `TODO`, or `...`, you MUST output "[Disagree]". (Note: even comments containing "pass"/"passing" can cause automated completeness checks to fail.)

  manager_prompt: &manager_prompt |-
    According to the Previous Solution and the Previous Sentences, select the most appropriate Critic from a specific Role and output the Role.
    {task_description}
    # Previous Solution
    The solution you gave in the last step is:
    {former_solution}

    # Critics
    There are some critics on the above solution:
    ```
    {critic_opinions}
    ```

    # Previous Sentences
    The previous sentences in the previous rounds is:
    {previous_sentence}

  executor_prepend_prompt: &executor_prepend_prompt |-
    You are an experienced program tester. Your team is trying to implement the following software:
    {task_description}

  executor_append_prompt: &executor_append_prompt |-
    The solution has been written to `tmp/main.py`. You are going to write unit tests for this solution.

    You MUST respond with a single JSON object with the following fields:

    {
      "thought": "your high-level thought about how to test this software",
      "reasoning": "your detailed reasoning on the testing cases",
      "criticism": "constructive self-criticism of your testing strategy",
      "file_path": "the path to write your testing code, e.g., tmp/test_main.py",
      "code": "the complete testing code in Python",
      "command": "the shell command to execute your testing code, e.g., python tmp/test_main.py"
    }

    Requirements:
    - Make sure the testing code is valid Python and can run the solution in `tmp/main.py`.
    - In the test code, import from `main` (e.g., `import main` or `from main import ...`).
    - Avoid flaky tests; do not use randomness unless seeded.
    - Do NOT add any other top-level fields besides the six specified.
    - Do NOT output any text before or after the JSON object.

  evaluator_prepend_prompt: &evaluator_prepend_prompt |-
    # Problem
    Implement the following software:
    {task_description}
    
    # Experts
    The experts recruited in this turn includes:
    {roles}

    # Writer's Solution:
    {solution}

    # Tester's Feedback:
    {result}

  evaluator_append_prompt: &evaluator_append_prompt |-
    You MUST respond with a single JSON object with the following fields:

    {
      "score": 0 or 1,
      "advice": "your detailed advice on how to correct the solution, and your suggestion on what experts should recruit in the next round"
    }

    Requirements:
    - "score" MUST be 0 if the solution is incorrect, and 1 if it is correct.
    - "advice" MUST be a natural language explanation.
    - Do NOT add any other top-level fields.
    - Do NOT output any text before or after the JSON object.


environment:
  env_type: task-basic
  max_turn: *max_turn
  rule:
    role_assigner:
      type: role_description
      cnt_agents: *cnt_agents
    decision_maker:
      type: vertical-solver-first
    executor:
      type: code-test
    evaluator:
      type: basic

agents:
  - agent_type: role_assigner
    name: role assigner
    max_retry: 1000
    prepend_prompt_template: *role_assigner_prepend_prompt
    append_prompt_template: *role_assigner_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 0
      max_tokens: 512

  - agent_type: solver
    name: Planner
    max_retry: 1000
    prepend_prompt_template: *solver_prepend_prompt
    append_prompt_template: *solver_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 0
      max_tokens: 2048

  - agent_type: critic
    name: Critic 1
    max_retry: 1000
    role_description: |-
      Waiting to be assigned.
    prepend_prompt_template: *critic_prepend_prompt
    append_prompt_template: *critic_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 0
      max_tokens: 1024

  - agent_type: executor
    name: Executor
    max_retry: 1000
    prepend_prompt_template: *executor_prepend_prompt
    append_prompt_template: *executor_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: gpt-4
      temperature: 0
      max_tokens: 1024

  - agent_type: evaluator
    name: Evaluator
    max_retry: 1000
    role_description: |-
      Evaluator
    prepend_prompt_template: *evaluator_prepend_prompt
    append_prompt_template: *evaluator_append_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: gpt-4
      temperature: 0.3
      max_tokens: 1024

  - agent_type: manager
    name: Manager
    max_retry: 1000
    prompt_template: *manager_prompt
    memory:
      memory_type: chat_history
    llm:
      llm_type: gpt-4
      model: "gpt-4"
      temperature: 0
      max_tokens: 512
